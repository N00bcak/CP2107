\documentclass{article} % For LaTeX2e
\usepackage[final]{../colm2025_conference}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{booktabs}
\usepackage{soul}
\usepackage{cancel}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{tablefootnote}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
% \newtheorem{proof}{Proof}[section]

\usepackage{lineno}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}


\title{Week 9: Why RL x LLM Works}

\author{\textbf{BEH} Chuen Yang}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\ifcolmsubmission
\linenumbers
\fi

\maketitle

% We are now well prepared to dive into RL x LLM! 
% Please read the following papers and write a summary to highlight the key 
% things that you find important for the success of RL x LLM:
% DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (https://arxiv.org/abs/2501.12948)
% Understanding R1-Zero-Like Training: A Critical Perspective (https://arxiv.org/abs/2503.20783)
% In your summary you need to note the differences between Reinforcement Learning 
% from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR). 
% These two papers focus on the latter, which is a powerful tool to develop *reasoning* language models. 
% Also try to connect with any topics that we learned previously (MDP, policy gradient, etc.)


% Overall structure:
% 1. Background
%   - InstructGPT, Deepseek R1-Zero, and Dr. GRPO.
% 2. Why RL x LLM is a highly non-trivial endeavor
%   2a. Problems with LLMs
%   2b. Problems with RL
% 3. Some hypotheses on why RL post-training works.
%   3a. Non-episodic nature of RL
%   3b. Lower reward variance due to good initialization
%   3c. Improvements in baseline LLMs
% 4. Conclusion

\begin{abstract}
    This report pertains to the apparently unlikely success of Reinforcement Learning
    in post-training large language models (RL x LLM).
    We first briefly outline the differences between two common approaches to RL x LLM:
    Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards
    (RLVR). Then, we propose a few hypotheses on why RL x LLM is so successful 
    despite the multitude of challenges that should at first glance plague RL x LLM.
    It is hoped that this report informs readers intuitions on RL x LLM,
    and provides a foundation for further exploration of the topic.
\end{abstract}

\section{Introduction}

Ever since \cite{InstructGPT-2022} demonstrated the effectiveness of 
Reinforcement Learning with Human Feedback (RLHF) 
in post-training large language models (LLMs), 
there has been a surge of interest in applying RL to LLMs. 

Since then, RL has culminated in many technical marvels such as 
the Claude 4 Family (\cite{Anthropic-2025}), Tulu 3 (\cite{Lambert-et-al-2024}) 
and DeepSeek-R1-Zero (\cite{DeepSeek-2025}),
with the latter even being able to omit the need for expensive and unscalable
Supervised Fine-Tuning (SFT) data.

However, this apparent success of RL in LLM post-training 
is extremely surprising in and of itself. Therefore, this report aims to explore the reasons why,
\textbf{despite the multitude of challenges that RL should, in theory, 
face when applied to LLM post-training,
it nevertheless succeeds with aplomb in reality.}

\section{Types of RL x LLM}

Before we explore the mysterious success of RL x LLM,
we will first explore two common approaches to RL x LLM which
have been applied with mainstream success:

\subsection{Reinforcement Learning with Human Feedback (RLHF)}

As mentioned in \cite{beh-2025-b}, RLHF (\cite{Christiano-et-al-2017}) attempts 
to steer an LLM's output distribution to one whose outputs are more desirable to humans.
RLHF achieves this by training a reward model (RM) (a Bradley-Terry model 
(\cite{Bradley-and-Terry-1952}) parameterized by a neural network) to predict
the relative desirability of any output from the LLM, then 
performing RL in an armed bandit fashion (\cite{Sutton-and-Barto-1998})
to induce the LLM to output more desirable outputs.

This approach distinguishes itself from other approaches to RL x LLM
by its use of human feedback to train the RM, which technically allows the RM to
be trained on a wide variety of tasks. While this is a strength of RLHF,
the use of a neural network as the reward model also means that
the rollout process can be quite expensive, as the RM must be queried
for every output generated by the LLM. Moreover, as a mere approximation,
the RM can also be inaccurate, which can lead to suboptimal policies being learned
as a result of RLHF.

\subsection{Reinforcement Learning with Verifiable Rewards (RLVR)}
Again as mentioned in \cite{beh-2025-b}, RLVR is a more recent approach to RL x LLM
pioneered by \cite{Lambert-et-al-2024} and then further developed by \cite{DeepSeek-2025}.
In contrast to RLHF, RLVR attempts to steer an LLM's output distribution \textbf{not} 
towards human preference, but instead towards a distribution whose outputs
are verifiably correct according to some task-specific criteria.

This simplicity of RLVR allows it to easily deliver results when applied to 
a very small subset of tasks, such as coding, mathematics, formatting, and reasoning tasks.
However, RLVR is consequentially much more limited in scope than RLHF,
and while effective, it is therefore less wieldy as a post-training technique.

\section{Why RL x LLM Seems Rather Difficult}

It is known that solving high-dimensional problems using RL is notoriously difficult 
(\cite{Sutton-and-Barto-1998, Jones-2021}),
especially when considering the set of all possible LLM outputs over today's
truly monstrous context window sizes and vocabularies (\cite{Anthropic-2025, DeepSeek-2025, Google-2025}).
Taking LLMs specifically into consideration,
this is because the set of possible states and actions both grow \textit{at least} exponentially
in the context window size. \footnote{
    For some intuition, consider the simplest case, where
    LLM generation is treated as an armed bandit problem (\cite{Sutton-and-Barto-1998}).
    With an input sequence of length $n$,
    and an output sequence of length $m$.
    If the vocabulary set is $V$,
    then we have $|V|^n$ possible input states,
    and $|V|^m$ possible output actions.
}

This compounds to the already monumental difficulty of getting 
distinguishable advantages between different outputs
in both RLHF (which in its base form uses some variation of the Bradley-Terry model (\cite{Bradley-and-Terry-1952}))
as well as DeepSeek-R1-Zero's take on RLVR (\cite{DeepSeek-2025}), 
for the model could output thousands of tokens and receive but a small reward signal
in the interval $[-1, 1]$. 

As \cite{Salimans-and-Chen-2018} have empirically demonstrated,
even relatively advanced RL algorithms such as Proximal Policy Optimization (PPO) 
(\cite{Schulman-et-al-2017}) have a difficult time learning a good policy
within reasonable wall-clock times
when reward signals are sparse / delayed, and / or when 
the state and action spaces are high-dimensional.

\subsection{Aside: KL Divergence Penalty}
Moreover, RL x LLM techniques often utilise a KL divergence penalty
to disincentivize the RL post-trained LLM from deviating too far from the base LLM
(\cite{InstructGPT-2022, Lambert-et-al-2024, DeepSeek-2025}).

While there are clear motivations for doing so (i.e.
preserving the generation quality of the base LLM and preventing catastrophic forgetting),
the KL divergence penalty actively acts against the RL post-training process
when it comes to exploration, as it discourages the RL post-trained LLM
from exploring diverse outputs that may be necessary to learn a good policy.

This is another factor which exacerbates the apparent difficulty of RL x LLM.

\section{Why Does RL x LLM Work Anyway?}

Yet, despite the above challenges, RL post-training of LLMs has been shown to work
in practice (\cite{InstructGPT-2022}). Most surprisingly,
RL post-training has been increasingly favored by frontier labs
(\cite{Anthropic-2025, Lambert-et-al-2024}), and shown to succeed
under even harsher conditions, such as in DeepSeek-R1-Zero (\cite{DeepSeek-2025}),
where RL post-training is done without any SFT data to serve as a backbone,
and where the optimization algorithm used (GRPO) discards an important
variance reduction technique in classical RL: advantage 
estimation (\cite{Schulman-et-al-2017, Sutton-and-Barto-1998}) using a critic network.

We attempt to explain why in the following subsections.

\subsection{Lower Reward Variance Due to Good Initialization}

The best possible rationalization for the success of RL post-training
is to think of pre-training as a sophisticated, high-quality initialization
procedure for the RL post-training process.

Another aspect of the RL optimization process that makes RL unwieldy 
for tasks with sufficient complexity is its empirical sensitivity to 
weight initialization (\cite{Jones-2021, Andrychowicz-et-al-2020}).

By pre-training on a massive corpus of text data,
the base LLM is able to first figure out the structure of language,
which significantly prunes the set of possible outputs
to a much smaller, sensible set of token sequences.
In other words, the base LLM learns a good initialization for the RL 
post-training process through pre-training.

Hence, the base LLM's action space in terms of the RL post-training process
becomes much smaller. Not only is this more practical to explore, 
but it also means that the reward variance is much lower, making the problem of
learning a good policy much easier than would be suggested by a simple
numerical analysis of the action space size.

\subsection{Improvements in Base Language Models}

Pre-trained language models used in RL post-training
have also improved significantly over the years. From the first base models
like GPT-2-XL (\cite{Radford-et-al-2019}) to modern base LLMs of comparable size 
like Qwen-2.5-1.5B (\cite{Qwen-et-al-2025}), base LLMs have become
much more performant across a wide variety of tasks (few-shot) (\cite{open-llm-leaderboard-v2}) 
like Instruction-Following (\cite{IFEval, BBH}), Language Understanding (\cite{MMLU-Pro}),
Mathematics (\cite{MATH-5}), question-answering (\cite{GPQA, BBH}), and even 
reasoning (\cite{MATH-5, MuSR}).
\footnote{
    While common reasons for this phenomenon include improvements in
    architecture design, training data quality and quantity,
    and training techniques,
    it is worth noting that newer base LLMs may also have been trained on
    text chunks generated by other LLMs doing reasoning tasks.
    It is possible that some part of base LLMs' performance improvements
    come from "cheating" in this sense.
}

Since it is already believed that the base LLM can be thought of as 
a high-quality initialization for the RL post-training process,
the improvements in base LLMs themselves would
rather straightforwardly entail greater ease (and indeed, performance)
in the RL post-training process.

\subsection{KL Divergence Penalty Creates Stable Exploration}

Considering what was mentioned in the previous section,
this hypothesis may come across as a bit counterintuitive.
However, \cite{Vieillard-et-al-2020} find that KL regularization,
when used with RL algorithms like Value Iteration (\cite{Bellman-1957}),
can actually help to stabilize exploration in RL.

Where RL x LLM in the armed-bandit case is concerned, 
the authors notice that KL regularization helps to average (and therefore
mitigate) any bias or random error in the value function updates,
resulting in a more stable exploration process.

However, it is worth noting that the exact mechanisms by which this occurs 
are not fully understood, especially because the authors' assumptions do not hold up 
when neural network approximators are used.
Nevertheless, it is possible that
their hypothesis in the simple case still holds up in the context of Deep RL.

\bibliographystyle{../colm2025_conference}
\bibliography{wk9}

\end{document}